# Docker Compose - Machine A (32GB GPU)
# Services: vllm-completions, vllm-ocr

x-gpu-config: &gpu-config
  GPU_TOTAL_GB: 32

services:
  vllm-completions:
    image: vllm/vllm-openai:latest
    container_name: vllm-completions
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        GPU_UTIL=$$(python3 -c "print($$VLLM_COMPLETIONS_GPU_GB / $$GPU_TOTAL_GB)")
        echo "GPU_UTIL=$$GPU_UTIL ($$VLLM_COMPLETIONS_GPU_GB / $$GPU_TOTAL_GB GB)"
        exec vllm serve $$MODEL_COMPLETIONS_NAME \
          --port 8101 \
          --host 0.0.0.0 \
          --gpu-memory-utilization $$GPU_UTIL \
          --trust-remote-code \
          --disable-log-requests \
          --enable-prefix-caching \
          --max-num-seqs $$VLLM_COMPLETIONS_MAX_NUM_SEQS \
          --max-num-batched-tokens $$VLLM_COMPLETIONS_MAX_BATCHED_TOKENS \
          --max-model-len $$VLLM_COMPLETIONS_MAX_MODEL_LEN \
          --dtype auto \
          --enforce-eager \
          --limit-mm-per-prompt.video 0
    ports:
      - "0.0.0.0:8101:8101"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - VLLM_LOGGING_LEVEL=INFO
      - HF_HOME=/root/.cache/huggingface
      - OMP_NUM_THREADS=1
      # Model config
      - MODEL_COMPLETIONS_NAME=Qwen/Qwen3-VL-4B-Instruct
      - GPU_TOTAL_GB=32
      - VLLM_COMPLETIONS_GPU_GB=15
      - VLLM_COMPLETIONS_MAX_NUM_SEQS=64
      - VLLM_COMPLETIONS_MAX_BATCHED_TOKENS=16384
      - VLLM_COMPLETIONS_MAX_MODEL_LEN=8192
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8101/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 90s
    networks:
      - vllm-network
    restart: unless-stopped
    shm_size: '4gb'

  vllm-ocr:
    image: vllm/vllm-openai:latest
    container_name: vllm-ocr
    depends_on:
      vllm-completions:
        condition: service_healthy
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        GPU_UTIL=$$(python3 -c "print($$VLLM_OCR_GPU_GB / $$GPU_TOTAL_GB)")
        echo "GPU_UTIL=$$GPU_UTIL ($$VLLM_OCR_GPU_GB / $$GPU_TOTAL_GB GB)"
        exec vllm serve $$MODEL_OCR_NAME \
          --port 8102 \
          --host 0.0.0.0 \
          --gpu-memory-utilization $$GPU_UTIL \
          --logits-processors vllm.model_executor.models.deepseek_ocr:NGramPerReqLogitsProcessor \
          --trust-remote-code \
          --disable-log-requests \
          --max-num-seqs $$VLLM_OCR_MAX_NUM_SEQS \
          --max-model-len $$VLLM_OCR_MAX_MODEL_LEN \
          --dtype auto \
          --enforce-eager \
          --enable-request-id-headers \
          --limit-mm-per-prompt '{"image":1}' \
          --mm-processor-cache-gb 0
    ports:
      - "0.0.0.0:8102:8102"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - VLLM_LOGGING_LEVEL=INFO
      - HF_HOME=/root/.cache/huggingface
      - VLLM_MEDIA_URL_ALLOW_REDIRECTS=0
      # Model config
      - MODEL_OCR_NAME=tencent/HunyuanOCR
      - GPU_TOTAL_GB=32
      - VLLM_OCR_GPU_GB=5
      - VLLM_OCR_MAX_NUM_SEQS=64
      - VLLM_OCR_MAX_MODEL_LEN=8192
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8102/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 120s
    networks:
      - vllm-network
    restart: unless-stopped
    shm_size: '3gb'

  vllm-asr:
    build:
      context: ./qwen3-asr
      dockerfile: Dockerfile
    image: qwen3-asr-transformers:latest
    container_name: vllm-asr
    depends_on:
      vllm-ocr:
        condition: service_healthy
    ports:
      - "0.0.0.0:8103:8103"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      # Mount serve.py for development (avoids rebuild)
      - ./qwen3-asr/serve.py:/app/serve.py:ro
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - HF_HOME=/root/.cache/huggingface
      # Model config - Options: Qwen/Qwen3-ASR-1.7B (more accurate) or Qwen/Qwen3-ASR-0.6B (faster, less VRAM)
      - ASR_MODEL=Qwen/Qwen3-ASR-0.6B
      - ALIGNER_MODEL=Qwen/Qwen3-ForcedAligner-0.6B
      - DEVICE_MAP=cuda:0
      - DTYPE=bfloat16
      - MAX_BATCH_SIZE=32
      - MAX_NEW_TOKENS=256
      - SERVER_HOST=0.0.0.0
      - SERVER_PORT=8103
      # Gunicorn production settings
      - GUNICORN_WORKERS=1
      - GUNICORN_THREADS=4
      - GUNICORN_TIMEOUT=300
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8103/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 120s
    networks:
      - vllm-network
    restart: unless-stopped
    shm_size: '4gb'

networks:
  vllm-network:
    external: true
    name: vllm-shared-network
