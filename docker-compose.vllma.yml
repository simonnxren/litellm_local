# Docker Compose - Machine A (32GB GPU)
# Services: vllm-completions, vllm-ocr, vllm-asr

services:
  vllm-completions:
    image: vllm-blackwell-nightly:latest
    container_name: vllm-completions
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        GPU_UTIL=$$(python3 -c "print($$VLLM_COMPLETIONS_GPU_GB / $$GPU_TOTAL_GB)")
        echo "GPU_UTIL=$$GPU_UTIL ($$VLLM_COMPLETIONS_GPU_GB / $$GPU_TOTAL_GB GB)"
        exec vllm serve $$MODEL_COMPLETIONS_NAME \
          --port 8101 \
          --host 0.0.0.0 \
          --gpu-memory-utilization $$GPU_UTIL \
          --trust-remote-code \
          --disable-log-requests \
          --enable-prefix-caching \
          --enable-chunked-prefill \
          --max-num-seqs $$VLLM_COMPLETIONS_MAX_NUM_SEQS \
          --max-num-batched-tokens $$VLLM_COMPLETIONS_MAX_BATCHED_TOKENS \
          --max-model-len $$VLLM_COMPLETIONS_MAX_MODEL_LEN \
          --dtype auto \
          --enforce-eager \
          --limit-mm-per-prompt.video 0
    ports:
      - "0.0.0.0:8101:8101"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - VLLM_LOGGING_LEVEL=INFO
      - HF_HOME=/root/.cache/huggingface
      # Model config
      - MODEL_COMPLETIONS_NAME=Qwen/Qwen3-VL-4B-Instruct
      - GPU_TOTAL_GB=32
      - VLLM_COMPLETIONS_GPU_GB=15
      - VLLM_COMPLETIONS_MAX_NUM_SEQS=64
      - VLLM_COMPLETIONS_MAX_BATCHED_TOKENS=16384
      - VLLM_COMPLETIONS_MAX_MODEL_LEN=8192
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8101/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 90s
    networks:
      - vllm-network
    restart: unless-stopped
    ipc: host
    stop_grace_period: 30s

  vllm-ocr:
    image: vllm-blackwell-nightly:latest
    container_name: vllm-ocr
    depends_on:
      vllm-completions:
        condition: service_healthy
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        GPU_UTIL=$$(python3 -c "print($$VLLM_OCR_GPU_GB / $$GPU_TOTAL_GB)")
        echo "GPU_UTIL=$$GPU_UTIL ($$VLLM_OCR_GPU_GB / $$GPU_TOTAL_GB GB)"
        exec vllm serve $$MODEL_OCR_NAME \
          --port 8102 \
          --host 0.0.0.0 \
          --gpu-memory-utilization $$GPU_UTIL \
          --trust-remote-code \
          --disable-log-requests \
          --max-num-seqs $$VLLM_OCR_MAX_NUM_SEQS \
          --max-num-batched-tokens $$VLLM_OCR_MAX_BATCHED_TOKENS \
          --max-model-len $$VLLM_OCR_MAX_MODEL_LEN \
          --dtype auto \
          --enforce-eager
    ports:
      - "0.0.0.0:8102:8102"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - VLLM_LOGGING_LEVEL=INFO
      - HF_HOME=/root/.cache/huggingface
      - OMP_NUM_THREADS=1
      # Model config
      - MODEL_OCR_NAME=zai-org/GLM-OCR
      - GPU_TOTAL_GB=32
      - VLLM_OCR_GPU_GB=4
      - VLLM_OCR_MAX_NUM_SEQS=64
      - VLLM_OCR_MAX_BATCHED_TOKENS=8192
      - VLLM_OCR_MAX_MODEL_LEN=4096
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8102/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 120s
    networks:
      - vllm-network
    restart: unless-stopped
    ipc: host
    stop_grace_period: 30s

  vllm-asr:
    build:
      context: ./qwen3-asr
      dockerfile: Dockerfile
    image: qwen3-asr-transformers:latest
    container_name: vllm-asr
    depends_on:
      vllm-ocr:
        condition: service_healthy
    command: ["gunicorn", "-w", "1", "-b", "0.0.0.0:8103", "--timeout", "300", "--graceful-timeout", "120", "--keep-alive", "5", "--preload", "serve:create_app()"]
    ports:
      - "0.0.0.0:8103:8103"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      # Mount serve.py for development (avoids rebuild)
      - ./qwen3-asr/serve.py:/app/serve.py:ro
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - HF_HOME=/root/.cache/huggingface
      # Model config - Options: Qwen/Qwen3-ASR-1.7B (more accurate) or Qwen/Qwen3-ASR-0.6B (faster, less VRAM)
      - ASR_MODEL=Qwen/Qwen3-ASR-0.6B
      - ALIGNER_MODEL=Qwen/Qwen3-ForcedAligner-0.6B
      - DEVICE_MAP=cuda:0
      - DTYPE=bfloat16
      - MAX_BATCH_SIZE=32
      - MAX_NEW_TOKENS=256
      - SERVER_HOST=0.0.0.0
      - SERVER_PORT=8103
      # Gunicorn production settings
      - GUNICORN_WORKERS=1
      - GUNICORN_THREADS=4
      - GUNICORN_TIMEOUT=300
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8103/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 120s
    networks:
      - vllm-network
    restart: unless-stopped
    shm_size: '1gb'
    stop_grace_period: 120s

networks:
  vllm-network:
    external: true
    name: vllm-shared-network
