# Docker Compose - Full vLLM Setup
# 4 services: Embedding, Completions, OCR, Whisper (audio)
# Requires more GPU memory or multi-GPU setup

services:
  # vLLM Embedding Instance
  vllm-embedding:
    image: vllm/vllm-openai:latest
    container_name: vllm-embedding
    env_file:
      - .env
    command: >
      --model ${MODEL_EMBED_NAME}
      --task embed
      --port 8100
      --host 0.0.0.0
      --gpu-memory-utilization ${VLLM_EMBED_GPU_MEMORY:-0.15}
      --trust-remote-code
      --disable-log-requests
      --max-num-seqs ${VLLM_EMBED_MAX_NUM_SEQS:-256}
      --max-model-len 8192
      --max-num-batched-tokens ${VLLM_EMBED_MAX_BATCHED_TOKENS:-8192}
      --dtype auto
      --enforce-eager
      --enable-request-id-headers
      --pooler-config '{"pooling_type":"MEAN","normalize":true}'
    ports:
      - "0.0.0.0:${VLLM_EMBED_PORT}:8100"
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - VLLM_LOGGING_LEVEL=INFO
      - HF_HOME=/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8100/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 90s
    networks:
      - vllm-network
    restart: unless-stopped
    shm_size: '2gb'

  # vLLM Completions Instance
  vllm-completions:
    image: vllm/vllm-openai:latest
    container_name: vllm-completions
    env_file:
      - .env
    command: >
      --model ${MODEL_COMPLETIONS_NAME}
      --port 8101
      --host 0.0.0.0
      --gpu-memory-utilization ${VLLM_COMPLETIONS_GPU_MEMORY:-0.70}
      --trust-remote-code
      --disable-log-requests
      --enable-prefix-caching
      --max-num-seqs ${VLLM_COMPLETIONS_MAX_NUM_SEQS:-128}
      --max-num-batched-tokens ${VLLM_COMPLETIONS_MAX_BATCHED_TOKENS:-16384}
      --enable-chunked-prefill
      --max-model-len ${VLLM_COMPLETIONS_MAX_MODEL_LEN:-8192}
      --dtype auto
      --enforce-eager
      --enable-request-id-headers
      --enable-force-include-usage
      --enable-prompt-tokens-details
      --scheduling-policy ${VLLM_COMPLETIONS_SCHEDULER_POLICY:-fcfs}
      --reasoning-parser deepseek_r1
      --enable-auto-tool-choice
      --tool-call-parser hermes
      --guided-decoding-backend outlines
      --max-logprobs 20
      --kv-cache-dtype fp8
      --disable-frontend-multiprocessing
    ports:
      - "0.0.0.0:${VLLM_COMPLETIONS_PORT}:8101"
    volumes:
      - ${HF_CACHE_DIR:-~/.cache/huggingface}:/root/.cache/huggingface
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - VLLM_LOGGING_LEVEL=INFO
      - HF_HOME=/root/.cache/huggingface
    depends_on:
      vllm-embedding:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8101/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 90s
    networks:
      - vllm-network
    restart: unless-stopped
    shm_size: '4gb'

  # vLLM OCR Instance
  vllm-ocr:
    image: vllm/vllm-openai:latest
    container_name: vllm-ocr
    env_file:
      - .env
    command: >
      --model ${MODEL_OCR_NAME}
      --port 8102
      --host 0.0.0.0
      --gpu-memory-utilization ${VLLM_OCR_GPU_MEMORY:-0.20}
      --logits-processors vllm.model_executor.models.deepseek_ocr:NGramPerReqLogitsProcessor
      --trust-remote-code
      --disable-log-requests
      --max-num-seqs ${VLLM_OCR_MAX_NUM_SEQS:-64}
      --max-model-len ${VLLM_OCR_MAX_MODEL_LEN:-8192}
      --dtype auto
      --enforce-eager
      --enable-request-id-headers
      --limit-mm-per-prompt '{"image":1}'
      --mm-processor-cache-gb 0
    ports:
      - "0.0.0.0:${VLLM_OCR_PORT}:8102"
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - VLLM_LOGGING_LEVEL=INFO
      - HF_HOME=/root/.cache/huggingface
      - VLLM_MEDIA_URL_ALLOW_REDIRECTS=0
    depends_on:
      vllm-completions:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8102/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 120s
    networks:
      - vllm-network
    restart: unless-stopped
    shm_size: '3gb'

  # vLLM Whisper (Audio Transcription) Instance
  vllm-whisper:
    image: vllm/vllm-openai:latest
    container_name: vllm-whisper
    env_file:
      - .env
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        pip install --no-cache-dir 'vllm[audio]' && \
        vllm serve \
          --model ${MODEL_WHISPER_NAME} \
          --port 8103 \
          --host 0.0.0.0 \
          --gpu-memory-utilization ${VLLM_WHISPER_GPU_MEMORY:-0.10} \
          --trust-remote-code \
          --disable-log-requests \
          --max-num-seqs ${VLLM_WHISPER_MAX_NUM_SEQS:-64} \
          --max-model-len ${VLLM_WHISPER_MAX_MODEL_LEN:-448} \
          --dtype auto \
          --enforce-eager \
          --enable-request-id-headers
    ports:
      - "0.0.0.0:${VLLM_WHISPER_PORT}:8103"
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - VLLM_LOGGING_LEVEL=INFO
      - HF_HOME=/root/.cache/huggingface
    depends_on:
      vllm-ocr:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8103/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 90s
    networks:
      - vllm-network
    restart: unless-stopped
    shm_size: '2gb'

networks:
  vllm-network:
    driver: bridge
    name: vllm-shared-network
