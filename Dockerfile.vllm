# vLLM built from source on CUDA 13.0 for Blackwell (RTX 5090)
# Build: docker build -f Dockerfile.vllm -t vllm-blackwell:latest .
# Run: docker run --gpus all -p 8000:8000 -v ~/.cache/huggingface:/root/.cache/huggingface --ipc=host vllm-blackwell:latest --model Qwen/Qwen3-0.6B

FROM nvidia/cuda:13.0.0-devel-ubuntu24.04

LABEL maintainer="vllm-blackwell"
LABEL description="vLLM built from source with Blackwell (sm_100) support using CUDA 13.0"

ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV PIP_NO_CACHE_DIR=1

# CUDA environment for building
ENV CUDA_HOME=/usr/local/cuda
ENV PATH="${CUDA_HOME}/bin:${PATH}"
ENV LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}"

# Build config - target Blackwell (sm_100)
ENV TORCH_CUDA_ARCH_LIST="10.0"
ENV MAX_JOBS=8

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.12 \
    python3.12-dev \
    python3.12-venv \
    python3-pip \
    curl \
    git \
    wget \
    build-essential \
    cmake \
    ninja-build \
    ccache \
    libxcb1 \
    libx11-6 \
    libxext6 \
    libgl1 \
    libglib2.0-0 \
    libnuma-dev \
    && rm -rf /var/lib/apt/lists/*

# Make python3.12 the default
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.12 1 \
    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.12 1

# Install uv
RUN curl -LsSf https://astral.sh/uv/install.sh | sh
ENV PATH="/root/.local/bin:$PATH"

# Create venv
RUN uv venv --python 3.12 /opt/venv --seed
ENV VIRTUAL_ENV=/opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Install PyTorch with CUDA 13.0 first
RUN uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130

# Clone vLLM from source (latest main)
WORKDIR /build
RUN git clone --depth 1 https://github.com/vllm-project/vllm.git

# Install build dependencies
WORKDIR /build/vllm
RUN uv pip install -r requirements/build.txt

# Build and install vLLM from source
RUN uv pip install -e . --no-build-isolation -v

# Install latest transformers from source (needed for new model architectures like glm_ocr)
RUN uv pip install git+https://github.com/huggingface/transformers.git

# Install additional packages
RUN uv pip install nvidia-ml-py

# Verify installation
RUN python -c "import vllm; print(f'vLLM: {vllm.__version__}')" \
    && python -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA built: {torch.version.cuda}')" \
    && nvcc --version

WORKDIR /workspace
EXPOSE 8000 8001 8002
ENTRYPOINT ["python", "-m", "vllm.entrypoints.openai.api_server"]
