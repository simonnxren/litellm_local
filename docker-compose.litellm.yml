# Docker Compose - LiteLLM Gateway
# OpenAI-compatible API gateway routing to local vLLM services (nightly stack)
#
# Services mapped from docker-compose.vllm_cu130_nightly.yml:
# - Embedding: Qwen3-VL-Embedding-2B-FP8 (Port 8090)
# - Completions: Qwen3-VL-4B-Instruct-FP8 (Port 8070)
# - OCR: GLM-OCR (Port 8080)
# - ASR: Qwen3-ASR-1.7B (Port 8000)

services:
  litellm:
    image: ghcr.io/berriai/litellm:main-stable
    container_name: litellm-gateway
    ports:
      - "0.0.0.0:8200:4000"
    configs:
      - source: litellm_config
        target: /app/config.yaml
    command: ["--config", "/app/config.yaml", "--port", "4000", "--num_workers", "8"]
    environment:
      - SEPARATE_HEALTH_APP=1
    networks:
      - vllm-network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:4000/health/liveliness', timeout=3).read()"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 15s
    restart: unless-stopped

configs:
  litellm_config:
    content: |
      model_list:
        # EMBEDDING SERVICE (Port 8090)
        - model_name: embedding
          litellm_params:
            model: hosted_vllm/shigureui/Qwen3-VL-Embedding-2B-FP8
            api_base: http://host.docker.internal:8090/v1
            api_key: "not-needed"
          model_info:
            mode: embedding

        - model_name: Qwen/Qwen3-VL-Embedding-2B-FP8
          litellm_params:
            model: hosted_vllm/shigureui/Qwen3-VL-Embedding-2B-FP8
            api_base: http://host.docker.internal:8090/v1
            api_key: "not-needed"
          model_info:
            mode: embedding

        # COMPLETIONS SERVICE (Port 8070)
        - model_name: completions
          litellm_params:
            model: hosted_vllm/Qwen/Qwen3-VL-4B-Instruct-FP8
            api_base: http://host.docker.internal:8070/v1
            api_key: "not-needed"
            supports_response_schema: true
          model_info:
            mode: chat
            supports_function_calling: true

        - model_name: Qwen/Qwen3-VL-4B-Instruct-FP8
          litellm_params:
            model: hosted_vllm/Qwen/Qwen3-VL-4B-Instruct-FP8
            api_base: http://host.docker.internal:8070/v1
            api_key: "not-needed"
            supports_response_schema: true
          model_info:
            mode: chat

        # OCR SERVICE (Port 8080)
        - model_name: ocr
          litellm_params:
            model: hosted_vllm/zai-org/GLM-OCR
            api_base: http://host.docker.internal:8080/v1
            api_key: "not-needed"
          model_info:
            mode: chat
            supports_vision: true

        - model_name: zai-org/GLM-OCR
          litellm_params:
            model: hosted_vllm/zai-org/GLM-OCR
            api_base: http://host.docker.internal:8080/v1
            api_key: "not-needed"
          model_info:
            mode: chat
            supports_vision: true

        # ASR SERVICE (Port 8000)
        - model_name: asr
          litellm_params:
            model: openai/Qwen/Qwen3-ASR-1.7B
            api_base: http://host.docker.internal:8000/v1
            api_key: "not-needed"
          model_info:
            mode: audio_transcription

        - model_name: Qwen/Qwen3-ASR-1.7B
          litellm_params:
            model: openai/Qwen/Qwen3-ASR-1.7B
            api_base: http://host.docker.internal:8000/v1
            api_key: "not-needed"
          model_info:
            mode: audio_transcription

      router_settings:
        routing_strategy: least-busy
        num_retries: 3
        timeout: 300
        allowed_fails: 1
        cooldown_time: 10

      litellm_settings:
        drop_params: true
        request_timeout: 300
        telemetry: false
        max_parallel_requests: 100
        cache: true
        cache_params:
          type: "local"
          ttl: 3600

      general_settings:
        store_model_in_db: false
        max_request_size_mb: 100

networks:
  vllm-network:
    external: true
    name: vllm-shared-network
