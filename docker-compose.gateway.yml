# =============================================================================
# LiteLLM Gateway — Unified API for all vLLM services
# =============================================================================
#
# Prerequisites: vLLM services running via docker-compose.vllm_cu130_nightly.yml
#
# Start:   docker compose -f docker-compose.gateway.yml up -d
# Stop:    docker compose -f docker-compose.gateway.yml down
# Logs:    docker logs litellm-gateway --tail 100 -f
# Health:  curl http://localhost:4000/health
# Models:  curl http://localhost:4000/v1/models
#
# The gateway exposes a single OpenAI-compatible endpoint on port 4000.
# All backend vLLM services are accessed via host networking (localhost).
# =============================================================================

services:
  litellm-gateway:
    image: ghcr.io/berriai/litellm:main-stable
    container_name: litellm-gateway
    # Host networking — gateway can reach vLLM services on localhost ports
    network_mode: host
    volumes:
      - ./litellm_config.yaml:/app/config.yaml:ro
    command:
      - --config
      - /app/config.yaml
      - --port
      - "4000"
      - --num_workers
      - "4"
    environment:
      # Uncomment to require API key authentication:
      # - LITELLM_MASTER_KEY=sk-litellm-master-key
      - LITELLM_LOG=INFO
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "-c",
          "import urllib.request; urllib.request.urlopen('http://localhost:4000/health/liveliness', timeout=5).read()",
        ]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
