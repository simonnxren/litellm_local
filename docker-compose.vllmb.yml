# Docker Compose - Machine B (8GB GPU)
# Services: vllm-embedding, vllm-asr

services:
  vllm-embedding:
    image: vllm/vllm-openai:latest
    container_name: vllm-embedding
    entrypoint: [ "/bin/bash", "-c" ]
    command:
      - |
        GPU_UTIL=$$(python3 -c "print($$VLLM_EMBED_GPU_GB / $$GPU_TOTAL_GB)")
        echo "GPU_UTIL=$$GPU_UTIL ($$VLLM_EMBED_GPU_GB / $$GPU_TOTAL_GB GB)"
        exec vllm serve $$MODEL_EMBED_NAME \
          --port 8100 \
          --host 0.0.0.0 \
          --gpu-memory-utilization $$GPU_UTIL \
          --trust-remote-code \
          --disable-log-requests \
          --max-num-seqs $$VLLM_EMBED_MAX_NUM_SEQS \
          --max-model-len 4096 \
          --max-num-batched-tokens $$VLLM_EMBED_MAX_BATCHED_TOKENS \
          --dtype auto \
          --enforce-eager \
          --enable-request-id-headers \
          --pooler-config '{"pooling_type":"MEAN","normalize":true}'
    ports:
      - "0.0.0.0:8100:8100"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - VLLM_LOGGING_LEVEL=INFO
      - HF_HOME=/root/.cache/huggingface
      # Model config
      - MODEL_EMBED_NAME=Qwen/Qwen3-VL-Embedding-2B
      - GPU_TOTAL_GB=8
      - VLLM_EMBED_GPU_GB=2
      - VLLM_EMBED_MAX_NUM_SEQS=256
      - VLLM_EMBED_MAX_BATCHED_TOKENS=8192
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8100/health" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 90s
    networks:
      - vllm-network
    restart: unless-stopped
    shm_size: '2gb'

networks:
  vllm-network:
    external: true
    name: vllm-shared-network
