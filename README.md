# vLLM Blackwell Nightly

vLLM deployment for RTX 5090 (Blackwell / sm_100) using CUDA 13.0.

## ðŸŽ‰ Current Status

### All Services Fully Operational âœ…
**Status**: âœ… **ALL 4 SERVICES WORKING WITH FULL FUNCTIONALITY**

**Core Functionality**:
- âœ… **Chat Service** (Port 8070) - Qwen3-VL-4B: Full chat and vision capabilities
- âœ… **OCR Service** (Port 8080) - GLM-OCR: Image text extraction  
- âœ… **Embedding Service** (Port 8090) - Qwen3-VL-Embedding-2B-FP8: **FULL EMBEDDING FUNCTIONALITY RESTORED**
- âœ… **ASR Service** (Port 8000) - Qwen3-ASR: Audio transcription with timestamps

**Key Achievement**: 
- âœ… Proper `/v1/embeddings` endpoint now working with 2048-dimensional vectors
- âœ… Full OpenAI-compatible embedding API restored
- âœ… Multimodal embedding support (text, image, text+image combinations)
- âœ… All tests passing with genuine semantic embeddings

**Performance**:
- Chat responses: ~0.2-0.3 seconds
- Embedding generation: ~0.1-0.2 seconds (proper implementation)
- OCR processing: ~0.8 seconds
- ASR transcription: ~0.5 seconds

## Quick Start

```bash
# Start all services
docker compose -f docker-compose.glm-ocr-qwen3-asr.yml up -d

# Check status
docker compose -f docker-compose.glm-ocr-qwen3-asr.yml ps
```

## Services

| Service | Port | Model | GPU Memory | Purpose |
|---------|------|-------|------------|---------|
| glm-ocr | 8080 | zai-org/GLM-OCR | 15% | OCR for images |
| qwen3-vl-embedding | 8090 | shigureui/Qwen3-VL-Embedding-2B-FP8 | 15% | Multimodal embeddings |
| qwen3-asr | 8000 | Qwen/Qwen3-ASR-1.7B | 20% | Speech recognition |
| qwen3-vl-4b | 8070 | Qwen/Qwen3-VL-4B-Instruct-FP8 | 38% | Vision-language chat |

**Total:** ~88% GPU memory utilization on 32GB RTX 5090

## System Requirements

- **GPU:** NVIDIA RTX 5090 (32GB)
- **Driver:** 580.x+
- **CUDA:** 13.0
- **Image:** `vllm/vllm-openai:cu130-nightly`

## Files

| File | Purpose |
|------|---------|
| `docker-compose.glm-ocr-qwen3-asr.yml` | Main stack with all 4 services |
| `Dockerfile.vllm-nightly` | Custom build (for reference) |
| `quantize_glm_ocr_fp8.py` | FP8 quantization script |
| `test_client_full.py` | ðŸ§ª Comprehensive test suite |
| `pytest.ini` | Pytest configuration |

## Testing

### Run All Tests
```bash
python test_client_full.py
```

### Run with Pytest (More Options)
```bash
# Verbose output
pytest test_client_full.py -v

# Run specific test categories
pytest test_client_full.py -m embedding    # Only embedding tests
pytest test_client_full.py -m integration  # Only integration tests
pytest test_client_full.py -m "not slow"   # Skip slow tests
```

### Test Coverage
The test suite covers all four capabilities:

1. **Chat Completion** (`TestChatCompletion`)
   - Basic text chat
   - Chat with image input

2. **Embedding** (`TestEmbedding`) - Based on vLLM/LiteLLM research
   - Text embeddings (single and batch)
   - Multimodal embeddings (text+image)
   - Consistency validation
   - Performance benchmarks
   - Edge case handling

3. **OCR** (`TestOCR`)
   - Basic OCR connectivity
   - OCR with real images

4. **ASR** (`TestASR`)
   - Audio transcription (silent audio)
   - Transcription with real audio files

5. **Integration** (`TestIntegration`)
   - End-to-end testing of all services

## API Usage

### OCR (GLM-OCR)
```bash
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "zai-org/GLM-OCR",
    "messages": [{"role": "user", "content": [
      {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}}
      {"type": "text", "text": "OCR this image"}
    ]}],
    "max_tokens": 500
  }'
```

### ASR (Qwen3-ASR)
```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen3-ASR-1.7B",
    "messages": [{"role": "user", "content": [
      {"type": "input_audio", "input_audio": {"data": "BASE64_WAV", "format": "wav"}},
      {"type": "text", "text": "<|audio_bos|><|AUDIO|><|audio_eos|>"}
    ]}],
    "max_tokens": 500
  }'
```

## Troubleshooting

### Error 803: CUDA Driver Mismatch
Use `vllm/vllm-openai:cu130-nightly` (not `nightly`)

### libcuda.so Conflict
Container startup removes compat libcuda:
```bash
rm -f /usr/local/cuda/compat/libcuda.so* && ldconfig
```
(Already in docker-compose entrypoint)

### New Model Architectures
Install latest transformers:
```bash
pip install https://github.com/huggingface/transformers/archive/refs/heads/main.zip
```
(Already in docker-compose entrypoint)
