# Docker Compose — LiteLLM Gateway for the MLX stack
#
# Routes OpenAI-compatible requests to the four MLX services running
# natively on the Mac host.  The gateway runs in Docker and reaches the
# host via `host.docker.internal`.
#
# Ports:  chat :8101 | ocr :8102 | asr :8103 | embedding :8100 | gateway :8200

services:
  litellm-mlx:
    image: ghcr.io/berriai/litellm:main-stable
    container_name: litellm-mlx
    ports:
      - "0.0.0.0:8200:4000"
    configs:
      - source: litellm_config
        target: /app/config.yaml
    command: ["--config", "/app/config.yaml", "--port", "4000", "--num_workers", "8"]
    environment:
      - SEPARATE_HEALTH_APP=1
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:4000/health/liveliness', timeout=3).read()"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 15s
    restart: unless-stopped

configs:
  litellm_config:
    content: |
      model_list:
        # ─── EMBEDDING  (MLX — host :8100) ──────────────────────────
        - model_name: embedding
          litellm_params:
            model: openai/embedding
            api_base: http://host.docker.internal:8100/v1
            api_key: "not-needed"
          model_info:
            mode: embedding

        # ─── CHAT / COMPLETIONS  (MLX — host :8101) ─────────────────
        - model_name: completions
          litellm_params:
            model: hosted_vllm/mlx-community/Qwen3-VL-8B-Instruct-8bit
            api_base: http://host.docker.internal:8101/v1
            api_key: "not-needed"
            supports_response_schema: true
          model_info:
            mode: chat
            supports_function_calling: true

        # ─── OCR / VISION  (MLX — host :8102) ───────────────────────
        # mlx_vlm serves /chat/completions (no /v1/ prefix),
        # so api_base points to the root — hosted_vllm/ appends
        # /chat/completions automatically.
        - model_name: ocr
          litellm_params:
            model: hosted_vllm/mlx-community/GLM-OCR-8bit
            api_base: http://host.docker.internal:8102
            api_key: "not-needed"
          model_info:
            mode: chat
            supports_vision: true

        # ─── ASR / AUDIO TRANSCRIPTION  (MLX — host :8103) ──────────
        - model_name: asr
          litellm_params:
            model: openai/mlx-community/whisper-large-v3-turbo
            api_base: http://host.docker.internal:8103/v1
            api_key: "not-needed"
          model_info:
            mode: audio_transcription

      router_settings:
        routing_strategy: least-busy
        num_retries: 3
        timeout: 300
        allowed_fails: 1
        cooldown_time: 10

      litellm_settings:
        drop_params: true
        request_timeout: 300
        telemetry: false
        max_parallel_requests: 100
        cache: true
        cache_params:
          type: "local"
          ttl: 3600

      general_settings:
        store_model_in_db: false
        max_request_size_mb: 100
