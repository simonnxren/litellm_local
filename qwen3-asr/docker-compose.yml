version: "3.8"

services:
  qwen3-asr:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        CUDA_VERSION: "12.8.0"
    image: qwen3-asr-vllm:latest
    container_name: qwen3-asr
    
    # GPU access
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    
    # Port mapping
    ports:
      - "8000:8000"
    
    # Environment configuration
    environment:
      - ASR_MODEL=Qwen/Qwen3-ASR-1.7B
      - ALIGNER_MODEL=Qwen/Qwen3-ForcedAligner-0.6B
      - GPU_MEMORY_UTILIZATION=0.8
      - MAX_BATCH_SIZE=32
      - MAX_NEW_TOKENS=2048
      - SERVER_HOST=0.0.0.0
      - SERVER_PORT=8000
    
    # Mount HuggingFace cache to avoid re-downloading models
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    
    # Shared memory for PyTorch
    shm_size: "8gb"
    
    # Healthcheck
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 30s
      start_period: 120s
      retries: 3
    
    # Default command: start vLLM server
    command: ["serve"]
    
    restart: unless-stopped

  # Alternative: Gradio Demo service
  qwen3-asr-demo:
    build:
      context: .
      dockerfile: Dockerfile
    image: qwen3-asr-vllm:latest
    container_name: qwen3-asr-demo
    profiles:
      - demo
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    
    ports:
      - "8000:8000"
    
    environment:
      - ASR_MODEL=Qwen/Qwen3-ASR-1.7B
      - ALIGNER_MODEL=Qwen/Qwen3-ForcedAligner-0.6B
      - GPU_MEMORY_UTILIZATION=0.7
      - MAX_BATCH_SIZE=8
      - MAX_NEW_TOKENS=2048
    
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    
    shm_size: "8gb"
    
    command: ["demo"]
    
    restart: unless-stopped

  # Streaming Demo service
  qwen3-asr-streaming:
    build:
      context: .
      dockerfile: Dockerfile
    image: qwen3-asr-vllm:latest
    container_name: qwen3-asr-streaming
    profiles:
      - streaming
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    
    ports:
      - "8000:8000"
    
    environment:
      - ASR_MODEL=Qwen/Qwen3-ASR-1.7B
      - GPU_MEMORY_UTILIZATION=0.9
    
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    
    shm_size: "8gb"
    
    command: ["demo-streaming"]
    
    restart: unless-stopped
