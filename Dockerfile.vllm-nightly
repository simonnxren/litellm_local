# =============================================================================
# vLLM Nightly - Blackwell (RTX 5090 / sm_100) - CUDA 13.0
# =============================================================================
# Purpose: Pre-built nightly vLLM wheels with latest AI model support
# GPU:     NVIDIA Blackwell (RTX 5090, sm_100)
# CUDA:    13.0 with devel base (needed for JIT: FlashInfer, DeepGEMM)
#
# Build:   docker build -f Dockerfile.vllm-nightly -t vllm-blackwell-nightly:latest .
# Run:     docker run --gpus all -p 8000:8000 \
#            -v ~/.cache/huggingface:/root/.cache/huggingface \
#            --ipc=host vllm-blackwell-nightly:latest \
#            --model Qwen/Qwen3-0.6B
# =============================================================================

# --------------- Build Arguments (version pinning) ---------------
ARG CUDA_VERSION=13.0.0

FROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu24.04

# Re-declare after FROM so it's available in the build stage
ARG CUDA_VERSION=13.0.0

LABEL maintainer="vllm-blackwell"
LABEL description="vLLM nightly with Blackwell (sm_100) support using CUDA 13.0"
LABEL cuda.version="${CUDA_VERSION}"

# --------------- Core Environment ---------------
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# CUDA environment
ENV CUDA_HOME=/usr/local/cuda
ENV PATH="${CUDA_HOME}/bin:${PATH}"
ENV LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}"

# CUDA 13.0: nvidia lib path restored to /usr/local/nvidia for host driver mounting
# (see https://github.com/vllm-project/vllm/issues/18859)
ENV LD_LIBRARY_PATH="/usr/local/nvidia/lib64:${LD_LIBRARY_PATH}"

# NVIDIA container runtime (ensures GPU access in all runtimes)
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility

# CUDA performance: lazy module loading reduces init time
ENV CUDA_MODULE_LOADING=LAZY

# Target Blackwell only - skip JIT for other architectures
ENV TORCH_CUDA_ARCH_LIST="10.0"

# uv package manager settings (from official vLLM Dockerfile)
ENV UV_HTTP_TIMEOUT=500
ENV UV_INDEX_STRATEGY="unsafe-best-match"
ENV UV_LINK_MODE=copy

# Fast model downloads from HuggingFace
ENV HF_HUB_ENABLE_HF_TRANSFER=1

# vLLM usage tracking
ENV VLLM_USAGE_SOURCE=production-docker-image

# --------------- System Dependencies ---------------
# Note: python3-pip and wget removed (uv handles all package management)
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.12 \
    python3.12-dev \
    python3.12-venv \
    curl \
    git \
    # X11/GL libs required for vision model image processing
    libxcb1 \
    libx11-6 \
    libxext6 \
    libgl1 \
    libglib2.0-0 \
    # NUMA support for multi-socket systems
    libnuma-dev \
    && rm -rf /var/lib/apt/lists/*

# Make python3.12 the default
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.12 1 \
    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.12 1

# Register CUDA compat libs (ptxjitcompiler, nvvm, gpucomp) but remove compat's
# libcuda.so â€” the host-mounted driver (e.g. 590.48) is newer than compat's (580.65)
# and compat's older libcuda doesn't support RTX 5090 Blackwell.
RUN echo "/usr/local/cuda/compat/" > /etc/ld.so.conf.d/cuda-compat.conf \
    && rm -f /usr/local/cuda/compat/libcuda.so* \
    && ldconfig

# --------------- Package Manager (uv) ---------------
RUN curl -LsSf https://astral.sh/uv/install.sh | sh
ENV PATH="/root/.local/bin:$PATH"

# Create virtual environment
RUN uv venv --python 3.12 /opt/venv --seed
ENV VIRTUAL_ENV=/opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# --------------- Python Dependencies ---------------
# Install vLLM nightly wheel for CUDA 13.0
# --prerelease=allow: nightly builds are pre-release (e.g. 0.15.2rc1.dev36)
# --index-url: prioritize the cu130 nightly index for vllm + torch
# --extra-index-url: fall back to PyPI for other dependencies
RUN uv pip install vllm torch --prerelease=allow \
    --index-url https://wheels.vllm.ai/nightly/cu130 \
    --extra-index-url https://pypi.org/simple \
    --extra-index-url https://download.pytorch.org/whl/cu130

# Install latest transformers from source (needed for bleeding-edge model architectures like glm_ocr)
# + hf_transfer for 5-10x faster model downloads
# + nvidia-ml-py for GPU monitoring (nvidia-smi Python bindings)
RUN uv pip install \
    git+https://github.com/huggingface/transformers.git \
    hf_transfer \
    nvidia-ml-py

# --------------- Verification ---------------
RUN python -c "import vllm; print(f'vLLM: {vllm.__version__}')" \
    && python -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.version.cuda}'); assert 'cu13' in torch.version.cuda.replace('.','') or int(torch.version.cuda.split('.')[0]) >= 13, f'Expected CUDA 13.x torch, got {torch.version.cuda}'" \
    && python -c "import hf_transfer; print('hf_transfer: OK')" \
    && nvcc --version

# --------------- Runtime Config ---------------
WORKDIR /workspace
EXPOSE 8000

# Health check: vLLM exposes /health endpoint on the API server
HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \
    CMD python -c "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')" || exit 1

# Use modern vllm serve entrypoint (replaces python -m vllm.entrypoints.openai.api_server)
ENTRYPOINT ["vllm", "serve"]
